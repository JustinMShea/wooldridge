---
title: "Introductory Econometrics Examples"
author: "Justin M Shea"
date: ' '
output:
    rmarkdown::html_vignette:
    toc: yes
vignette: >
  %\VignetteIndexEntry{Introductory Econometrics Examples}  
  %\VignetteEngine{knitr::rmarkdown}  
  %\VignetteEncoding{UTF-8}
---

\newpage

## Introduction

This vignette contains examples from every chapter of _Introductory Econometrics: A Modern Approach, 6e_ by Jeffrey M. Wooldridge. Each example illustrates how to load data, build econometric models, and compute estimates with **R**.

In addition, the **Appendix** cites good sources on using **R** for econometrics. 


Now, install and load the `wooldridge` package and lets get started!

```{r, echo = TRUE, eval = FALSE, warning=FALSE}
install.packages("wooldridge")
```

```{r, echo = TRUE, eval = TRUE, warning=FALSE, message=FALSE}
library(wooldridge)
```

```{r, echo=FALSE, eval=TRUE, warning=FALSE, message=FALSE}
library(stargazer)
library(knitr)
```


\newpage

## Chapter 2: The Simple Regression Model

###**`Example 2.10:` A Log Wage Equation**

Load the `wage1` data and check out the documentation.

```{r, message=FALSE, eval=FALSE}
data("wage1")
?wage1
```
These are data from the 1976 Current Population Survey, collected by Henry Farber when he and Wooldridge were colleagues at MIT in 1988.

First, make a scatter-plot of the two variables and look for possible patterns in the relationship between them.

```{r, tidy=TRUE}
plot(y = wage1$wage, x = wage1$educ, col = "blue", xaxt="n", frame = FALSE, main = "Wages vs. Education, 1976", xlab = "years of education", ylab = "Hourly wages")
axis(side = 1, at = c(0,6,12,18))
```

It appears that _**on average**_, more years of education, leads to higher wages.

The example in the text investigates what the _**percentage**_ change between wages and education might be. So, we must use the $log($`wage`$)$.

Build a linear model to estimate the relationship between the _log of wage_ (`lwage`) and _education_ (`educ`).

$$\widehat{log(wage)} = \beta_0 + \beta_1educ$$

```{r}
log_wage_model <- lm(lwage ~ educ, data = wage1)
```

Print the `summary` of the results.

```{r, echo = TRUE, eval = FALSE, warning=FALSE}
summary(log_wage_model)
```


```{r, results='asis', echo=FALSE, warning=FALSE, message=FALSE}
stargazer(log_wage_model, type = "html", single.row = TRUE, header = FALSE)
```

Plot the $log($`wage`$)$ vs `educ`, adding a line estimated with the linear model.

```{r, tidy=TRUE}
plot(y = wage1$lwage, x = wage1$educ, main = "A Log Wage Equation", xlab = "years of education", ylab = "log of average hourly wages",  xaxt="n", frame = FALSE)
axis(side = 1, at = c(0,6,12,18))
abline(log_wage_model, col = "blue", lwd=2)

```


\newpage

## Chapter 3: Multiple Regression Analysis: Estimation

###**`Example 3.2:` Hourly Wage Equation**

Plot the variables against `lwage`, and visually compare their distributions,
and slope of the simple regression line.

```{r, tidy=TRUE, fig.height=3}
par(mfrow=c(1,3))

plot(y = wage1$lwage, x = wage1$educ, xaxt="n", frame = FALSE, main = "years of education", xlab = "", ylab = "")
mtext(side=2, line=2.5, "Hourly wages", cex=1.25)
axis(side = 1, at = c(0,6,12,18))
abline(lm(lwage ~ educ, data=wage1), col = "blue", lwd=2)

plot(y = wage1$lwage, x = wage1$exper, xaxt="n", frame = FALSE, main = "years of experience", xlab = "", ylab = "")
axis(side = 1, at = c(0,12.5,25,37.5,50))
abline(lm(lwage ~ exper, data=wage1), col = "blue", lwd=2)

plot(y = wage1$lwage, x = wage1$tenure, xaxt="n", frame = FALSE, main = "years with employer", xlab = "", ylab = "")
axis(side = 1, at = c(0,11,22,33,44))
abline(lm(lwage ~ tenure, data=wage1), col = "blue", lwd=2)
```

Estimate the model regressing _education_, _experience_, and _tenure_ against _log(wage)_. 
The `wage1` data should still be in your working environment.

$$\widehat{log(wage)} = \beta_0 + \beta_1educ + \beta_3exper + \beta_4tenure$$

```{r}
hourly_wage_model <- lm(lwage ~ educ + exper + tenure, data = wage1)
```

Print the estimated model coefficients:
```{r, eval=FALSE}
coefficients(hourly_wage_model)
```

```{r, echo=FALSE}
kable(coefficients(hourly_wage_model), digits=4, col.names = "Coefficients", align = 'l')
```

Plot the coefficients, representing percentage impact of each variable on $log($`wage`$)$ for a quick comparison.

```{r}
barplot(100*hourly_wage_model$coefficients[-1], ylab = "Percentage", 
        main = "Coefficients of Hourly Wage Equation")
```

## Chapter 4: Multiple Regression Analysis: Inference

###**`Example 4.1` Hourly Wage Equation (continued)**

Using the same model estimated in **`example: 3.2`**, examine and compare the standard errors associated with each coefficient. Like the textbook, these are contained in parenthesis next to each associated coefficient.


```{r, eval=FALSE}
summary(hourly_wage_model)
```

```{r, results='asis', echo=FALSE, warning=FALSE, message=FALSE}
stargazer(type = "html", hourly_wage_model,  single.row = TRUE, header = FALSE)
```

For the years of experience variable, or `exper`, the coefficient and Standard Error is:

```{r}
round(summary(hourly_wage_model)$coefficients["exper",c("Estimate","Std. Error")], 4)
```

Plot the coefficients, representing the impact of each variable on $log($`wage`$)$ as well as their confidence intervals.

```{r}
coefficient <- 100*coef(hourly_wage_model)[-1]
 confidence <- 100*confint(hourly_wage_model, level = 0.95)[-1,]

graph <- drop(barplot(coefficient, ylim = range(c(confidence)),
              main = "Coefficients & 95% C.I. of variables on Hourly Wages"))  

arrows(graph, coefficient, graph, confidence[,1], angle=90, length=0.55, col="blue", lwd=1)
arrows(graph, coefficient, graph, confidence[,2], angle=90, length=0.55, col="blue", lwd=1)


```


###**`Example 4.7` Effect of Job Training on Firm Scrap Rates**

Load the `jtrain` data set. 

```{r, echo = TRUE, eval = TRUE, warning=FALSE, message=FALSE}
data("jtrain")
```

```{r, echo = TRUE, eval = FALSE, warning=FALSE}
?jtrain
```

From H. Holzer, R. Block, M. Cheatham, and J. Knott (1993), _Are Training Subsidies Effective? The Michigan Experience_, Industrial and Labor Relations Review 46, 625-636. The authors kindly provided the data.

First, use the `subset` function and it's argument by the same name to return
observations which occurred in **1987** and are not **union**. At the same time, use
the `select` argument to return only the variables of interest for this problem.

 

```{r}
jtrain_subset <- subset(jtrain, subset = (year == 1987 & union == 0),
                          select = c(year, union, lscrap, hrsemp, lsales, lemploy))
```

Next, test for missing values. One can "eyeball" these with R Studio's `View`
function, but a more precise approach combines the `sum` and `is.na` functions 
to return the total number of observations equal to `NA`.

```{r}
sum(is.na(jtrain_subset))
```

While `R`'s `lm` function will automatically remove missing `NA` values, eliminating
these manually will produce more clearly proportioned graphs for exploratory analysis.
Call the `na.omit` function to remove all missing values and assign the new 
`data.frame` object the name **`jtrain_clean`**.

```{r}
jtrain_clean <- na.omit(jtrain_subset)
```


Use `jtrain_clean` to plot the variables of interest against `lscrap`. Visually 
observe the respective distributions for each variable, and compare the slope of 
the simple regression lines.

```{r, tidy=TRUE, fig.height=3}
par(mfrow=c(1,3))

point_size <- 1.75

plot(y = jtrain_clean$lscrap, x = jtrain_clean$hrsemp, frame = FALSE, 
main = "Total (hours/employees) trained", ylab = "", xlab="", pch = 21, bg = "lightgrey", cex=point_size)
mtext(side=2, line=2, "Log(scrap rate)", cex=1.25)
abline(lm(lscrap ~ hrsemp, data=jtrain_clean), col = "blue", lwd=2)

plot(y = jtrain_clean$lscrap, x = jtrain_clean$lsales, frame = FALSE, main = "Log(annual sales $)", ylab = " ", xlab="", pch = 21, bg = "lightgrey", cex=point_size)
abline(lm(lscrap ~ lsales, data=jtrain_clean), col = "blue", lwd=2)

plot(y = jtrain_clean$lscrap, x = jtrain_clean$lemploy, frame = FALSE, main = "Log(# employees at plant)", ylab = " ", xlab="", pch = 21, bg = "lightgrey", cex=point_size)
abline(lm(lscrap ~ lemploy, data=jtrain_clean), col = "blue", lwd=2)
```


Now create the linear model regressing `hrsemp`(total hours training/total employees trained), the `lsales`(log of annual sales), and `lemploy`(the log of the number of the employees), against `lscrap`(the log of the scrape rate).

$$lscrap = \alpha + \beta_1 hrsemp + \beta_2 lsales + \beta_3 lemploy$$


```{r, tidy=TRUE}
linear_model <- lm(lscrap ~ hrsemp + lsales + lemploy, data = jtrain_clean)
```

Finally, print the complete summary diagnostics of the model.

```{r, eval=FALSE, warning=FALSE, message=FALSE}
summary(linear_model)
```

```{r, results='asis', echo=FALSE, warning=FALSE, message=FALSE}
stargazer(type = "html", linear_model, single.row = TRUE, header = FALSE)
```

Plot the coefficients, representing the impact of each variable on $log($`scrap`$)$ for a quick comparison. As you can observe, for some variables, the confidence intervals are wide.

```{r}
coefficient <- coef(linear_model)[-1]
 confidence <- confint(linear_model, level = 0.95)[-1,]

graph <- drop(barplot(coefficient, ylim = range(c(confidence)),
              main = "Coefficients & 95% C.I. of variables on Firm Scrap Rates"))  

arrows(graph, coefficient, graph, confidence[,1], angle=90, length=0.55, col="blue", lwd=2)
arrows(graph, coefficient, graph, confidence[,2], angle=90, length=0.55, col="blue", lwd=2)


```


## Chapter 5: Multiple Regression Analysis: OLS Asymptotics

###**`Example 5.1:` Housing Prices and Distance From an Incinerator**

Load the `hprice3` data set.

```{r}
data("hprice3")
```

In this example, we will need to construct a general `quality` variable from variables `rooms`, `area`, `land`, and `baths`. We can do this by creating a simple character string of the variables, assigning it the name `quality`.

```{r}
quality <- c("rooms", "area", "land", "baths")
```

$price = \alpha + \beta_1 dist + \beta_2 quality$ 

Estimate the model. Note the use of the `get` function wrapped around `quality`. This removes the `" "` and makes them visible as data objects associated with the `price3` data set.

```{r}
lm(price ~ dist + get(quality), data = hprice3)
```



###**`Example 5.3:` Economic Model of Crime**

Load the `crime1` data set.

```{r, message=FALSE, eval=FALSE}
data("crime1")
?crime1
```

From  J. Grogger (1991), _Certainty vs. Severity of Punishment_, Economic Inquiry 29, 297-309. 
Professor Grogger kindly provided a subset of the data he used in his article.

$narr86:$ number of times arrested, 1986.  
$pcnv:$ proportion of prior arrests leading to convictions.  
$avgsen:$ average sentence served, length in months.  
$tottime:$ time in prison since reaching the age of 18, length in months.  
$ptime86:$ months in prison during 1986.  
$qemp86:$ quarters employed, 1986.

Explore the data

```{r, tidy=TRUE, fig.height=3}
par(mfrow=c(1,3))

point_size <- 1.75

plot(y = jtrain_clean$lscrap, x = jtrain_clean$hrsemp, frame = FALSE, 
main = "Total (hours/employees) trained", ylab = "", xlab="", pch = 21, bg = "lightgrey", cex=point_size)
mtext(side=2, line=2, "Log(scrap rate)", cex=1.25)
abline(lm(lscrap ~ hrsemp, data=jtrain_clean), col = "blue", lwd=2)

plot(y = jtrain_clean$lscrap, x = jtrain_clean$lsales, frame = FALSE, main = "Log(annual sales $)", ylab = " ", xlab="", pch = 21, bg = "lightgrey", cex=point_size)
abline(lm(lscrap ~ lsales, data=jtrain_clean), col = "blue", lwd=2)

plot(y = jtrain_clean$lscrap, x = jtrain_clean$lemploy, frame = FALSE, main = "Log(# employees at plant)", ylab = " ", xlab="", pch = 21, bg = "lightgrey", cex=point_size)
abline(lm(lscrap ~ lemploy, data=jtrain_clean), col = "blue", lwd=2)
```

Estimate the model.


$$narr86 = \beta_0 + \beta_1pcnv + \beta_2avgsen + \beta_3tottime + \beta_4ptime86 + \beta_5qemp86 + \mu$$

```{r, tidy = TRUE}
restricted_model <- lm(narr86 ~ pcnv + ptime86 + qemp86, data = crime1)
```

Create a new variable `restricted_model_u` containing the residuals $\tilde{\mu}$ from the above regression.

```{r}
restricted_model_u <- restricted_model$residuals
```

Next, regress `pcnv, ptime86, qemp86, avgsen`, and `tottime`, against the residuals 
$\tilde{\mu}$ saved in `restricted_model_u`.

$$\tilde{\mu} = \beta_1pcnv + \beta_2avgsen + \beta_3tottime + \beta_4ptime86 + \beta_5qemp86$$


```{r, tidy = TRUE}
LM_u_model <- lm(restricted_model_u ~ pcnv + ptime86 + qemp86 + avgsen + tottime, data = crime1)
summary(LM_u_model)$r.square
```

$$LM = 2,725(0.0015)$$

```{r}
LM_test <- nobs(LM_u_model) * 0.0015
LM_test
```


```{r}
qchisq(1 - 0.10, 2)
```

The _p_-value is:
$$P(X^2_{2} > 4.09) \approx 0.129$$

```{r}
1-pchisq(LM_test, 2)
```

\newpage

## Chapter 6: Multiple Regression: Further Issues

###**`Example 6.1:` Effects of Pollution on Housing Prices, standardized.**

Data from _Hedonic Housing Prices and the Demand for Clean Air_, by Harrison, D. and D.L.Rubinfeld, Journal of Environmental Economics and Management 5, 81-102. Diego Garcia, a former Ph.D. student in economics at MIT, kindly provided these data, which he obtained from the book Regression Diagnostics: Identifying Influential Data and Sources of Collinearity, by D.A. Belsey, E. Kuh, and R. Welsch, 1990. New York: Wiley. 


$$price = \beta_0 + \beta_1nox + \beta_2crime + \beta_3rooms + \beta_4dist + \beta_5stratio + \mu$$

$price$: median housing price.

$nox$: Nitrous Oxide concentration; parts per million.

$crime$: number of reported crimes per capita.

$rooms$: average number of rooms in houses in the community.

$dist$: weighted distance of the community to 5 employment centers.

$stratio$: average student-teacher ratio of schools in the community.


$$\widehat{zprice} = \beta_1znox + \beta_2zcrime + \beta_3zrooms + \beta_4zdist + \beta_5zstratio$$

Load the `hprice2` data and view the documentation.


```{r, message=FALSE, eval=FALSE}
data("hprice2")
?hprice2
```


Estimate the coefficient with the usual `lm` regression model but this time, standardized coefficients by wrapping each variable with R's `scale` function:

```{r, tidy = TRUE}
housing_standard <- lm(scale(price)~0+scale(nox)+scale(crime)+scale(rooms)+scale(dist) + scale(stratio), data = hprice2)
```

```{r, results='asis', echo=FALSE, warning=FALSE, message=FALSE}
stargazer(type = "html",housing_standard,  single.row = TRUE, header = FALSE)
```


\newpage

###**`Example 6.2:` Effects of Pollution on Housing Prices, Quadratic Interactive Term**

Modify the housing model, adding a quadratic term in _rooms_: 

$$log(price) = \beta_0 + \beta_1log(nox) + \beta_2log(dist) + \beta_3rooms + \beta_4rooms^2 + \beta_5stratio + \mu$$
```{r}
housing_interactive <- lm(lprice ~ lnox + log(dist) + rooms+I(rooms^2) + stratio, data = hprice2)
```

Compare the results with the model from `example 6.1`.

```{r, results='asis', echo=FALSE, warning=FALSE, message=FALSE, tidy=TRUE}
stargazer(type = "html",housing_standard, housing_interactive, single.row = TRUE, header = FALSE)
```

\newpage

## Chapter 7: Multiple Regression Analysis with Qualitative Information 

###**`Example 7.4:` Housing Price Regression, Qualitative Binary variable**

This time, use the `hrprice1` data. Data collected from the real estate pages of the Boston Globe during 1990. These are homes that sold in the Boston, MA area.

```{r}
data("hprice1")
```

```{r, eval=FALSE}
?hprice1
```

$$\widehat{log(price)} = \beta_0 + \beta_1log(lotsize) + \beta_2log(sqrft) + \beta_3bdrms + \beta_4colonial $$

Estimate the coefficients of the above linear model on the `hprice` data set.

```{r, tidy=TRUE}
housing_qualitative <- lm(lprice ~ llotsize + lsqrft + bdrms + colonial, data = hprice1)
```

```{r, results='asis', echo=FALSE, warning=FALSE, message=FALSE, tidy=TRUE}
stargazer(type = "html",housing_qualitative,  single.row = TRUE, header = FALSE)
```

\newpage

## Chapter 8: Heteroskedasticity

###**`Example 8.9:` Determinants of Personal Computer Ownership**

$$\widehat{PC} = \beta_0 + \beta_1hsGPA + \beta_2ACT + \beta_3parcoll + \beta_4colonial $$
Christopher Lemmon, a former MSU undergraduate, collected these data from a survey he took of MSU students in Fall 1994. Load `gpa1` and create a new variable combining the `fathcoll` and `mothcoll`, into `parcoll`. This new column indicates if either parent went to college.

```{r, message=FALSE}
data("gpa1")

gpa1$parcoll <- as.integer(gpa1$fathcoll==1 | gpa1$mothcoll)


GPA_OLS <- lm(PC ~ hsGPA + ACT + parcoll, data = gpa1)
```

Calculate the weights and then pass them to the `weights` argument.

```{r}
weights <- GPA_OLS$fitted.values * (1-GPA_OLS$fitted.values)

GPA_WLS <- lm(PC ~ hsGPA + ACT + parcoll, data = gpa1, weights = 1/weights)
```

Compare the OLS and WLS model in the table below:

```{r, results='asis', echo=FALSE, warning=FALSE, message=FALSE, tidy=TRUE}
stargazer(type = "html",GPA_OLS, GPA_WLS,  single.row = TRUE, header = FALSE)
```


\newpage

## Chapter 9: More on Specification and Data Issues

###**`Example 9.8:` R&D Intensity and Firm Size**


$$rdintens = \beta_0 + \beta_1sales + \beta_2profmarg + \mu$$

From _Businessweek R&D Scoreboard_, October 25, 1991. Load the data and estimate the model.

```{r, message=FALSE}
data("rdchem")

all_rdchem <- lm(rdintens ~ sales + profmarg, data = rdchem)
```

Plotting the data reveals the outlier on the far right of the plot, which will skew the results of our model.

```{r, tidy=TRUE}
plot_title <- "FIGURE 9.1: Scatterplot of R&D intensity against firm sales"
x_axis <- "firm sales (in millions of dollars)"
y_axis <- "R&D as a percentage of sales"

plot(rdintens ~ sales, pch = 21, bg = "lightgrey", data = rdchem, main = plot_title, xlab = x_axis, ylab = y_axis)
```

So, we can estimate the model without that data point to gain a better understanding of how `sales` and `profmarg` describe `rdintens` for most firms. We can use the `subset` argument of the linear model function to indicate that we only want to estimate the model using data that is less than the highest sales.

```{r}
smallest_rdchem <- lm(rdintens ~ sales + profmarg, data = rdchem, 
                      subset = (sales < max(sales)))
```

The table below compares the results of both models side by side. By removing the outlier firm, $sales$ become a more significant determination of R&D expenditures.

```{r, results='asis', echo=FALSE, warning=FALSE, message=FALSE, tidy=TRUE}
stargazer(type = "html",all_rdchem, smallest_rdchem,  single.row = TRUE, header = FALSE)
```


\newpage

## Chapter 10: Basic Regression Analysis with Time Series Data

###**`Example 10.2:` Effects of Inflation and Deficits on Interest Rates**

$$\widehat{i3} = \beta_0 + \beta_1inf_t + \beta_2def_t$$
Data from the _Economic Report of the President, 2004_, Tables B-64, B-73, and B-79.

```{r, message=FALSE}
data("intdef")

tbill_model <- lm(i3 ~ inf + def, data = intdef)
```

```{r, results='asis', echo=FALSE, warning=FALSE, message=FALSE, tidy=TRUE}
stargazer(type = "html",tbill_model, single.row = TRUE, header = FALSE)
```


###**`Example 10.11:` Seasonal Effects of Antidumping Filings**

C.M. Krupp and P.S. Pollard (1999), _Market Responses to Antidumpting Laws: Some Evidence from the U.S. Chemical Industry_, Canadian Journal of Economics 29, 199-227. Dr. Krupp kindly provided the data. They are monthly data covering February 1978 through December 1988. 

```{r, message=FALSE, tidy=TRUE}
data("barium")

barium_imports <- lm(lchnimp ~ lchempi + lgas + lrtwex + befile6 + affile6 + afdec6, data = barium)
```

Estimate a new model, `barium_seasonal` which accounts for seasonality by adding dummy variables contained in the data. 

```{r, tidy=TRUE}
barium_seasonal <- lm(lchnimp ~ lchempi + lgas + lrtwex + befile6 + affile6 + afdec6 + feb + mar + apr + may + jun + jul + aug + sep + oct + nov + dec, data = barium)
```

Compare both models:

```{r, results='asis', echo=FALSE, warning=FALSE, message=FALSE, tidy=TRUE}
stargazer(type = "html",barium_imports, barium_seasonal,  single.row = TRUE, header = FALSE)
```

Now, compute the `anova` between the two models.

```{r}
barium_anova <- anova(barium_imports, barium_seasonal)
```

```{r, results='asis', echo=FALSE, warning=FALSE, message=FALSE, tidy=TRUE}
stargazer(type = "html",barium_anova,  single.row = TRUE, header = FALSE)
```

\newpage

## Chapter 11: Further Issues in Using OLS with with Time Series Data

###**`Example 11.7:` Wages and Productivity**


$$\widehat{log(hrwage_t)} = \beta_0 + \beta_1log(outphr_t) + \beta_2t + \mu_t$$
Data from the _Economic Report of the President, 1989_, Table B-47. The data are for the non-farm business sector.

```{r, message=FALSE}
data("earns")

wage_time <- lm(lhrwage ~ loutphr + t, data = earns)
```

```{r}
wage_diff <- lm(diff(lhrwage) ~ diff(loutphr), data = earns)
```

```{r, results='asis', echo=FALSE, warning=FALSE, message=FALSE, tidy=TRUE}
stargazer(type = "html",wage_time, wage_diff,  single.row = TRUE, header = FALSE)
```


\newpage

## Chapter 12: Serial Correlation and Heteroskedasticiy in Time Series Regressions

###**`Example 12.4`: Prais-Winsten Estimation in the Event Study**

```{r, tidy=TRUE}
data("barium")

barium_model <- lm(lchnimp ~ lchempi + lgas + lrtwex + befile6 + affile6 + afdec6, 
                   data = barium)
```


Load the `prais` package, use the `prais.winsten` function to estimate.

```{r, tidy=TRUE}
library(prais)
barium_prais_winsten <- prais.winsten(lchnimp ~ lchempi + lgas + lrtwex + befile6 + affile6 + afdec6, data = barium)
```

```{r, results='asis', echo=FALSE, warning=FALSE, message=FALSE, tidy=TRUE}
stargazer(type = "html", barium_model, single.row = TRUE, header = FALSE)
```

```{r}
barium_prais_winsten
```


\newpage

###**`Example 12.8:` Heteroskedasticity and the Efficient Markets Hypothesis**

These are Wednesday closing prices of value-weighted NYSE average, available in many publications. Wooldridge does not recall the particular source used when he collected these data at MIT, but notes probably the easiest way to get similar data is to go to the NYSE web site, [www.nyse.com](https://www.nyse.com/data-and-tech). 

$$return_t = \beta_0 + \beta_1return_{t-1} + \mu_t$$

```{r, message=FALSE, eval=FALSE}
data("nyse")
?nyse 
```
```{r}
return_AR1 <-lm(return ~ return_1, data = nyse)
```

$$\hat{\mu^2_t} = \beta_0 + \beta_1return_{t-1} + residual_t$$


```{r}
return_mu <- residuals(return_AR1)
mu2_hat_model <- lm(return_mu^2 ~ return_1, data = return_AR1$model)
```
```{r, results='asis', echo=FALSE, warning=FALSE, message=FALSE, tidy=TRUE}
stargazer(type = "html",return_AR1, mu2_hat_model,  single.row = TRUE, header = FALSE)
```


\newpage

###**`Example 12.9:` ARCH in Stock Returns**


$$\hat{\mu^2_t} = \beta_0 + \hat{\mu^2_{t-1}} + residual_t$$

We still have `return_mu` in the working environment so we can use it to create $\hat{\mu^2_t}$, (`mu2_hat`) and $\hat{\mu^2_{t-1}}$ (`mu2_hat_1`). Notice the use `R`'s matrix subset operations to perform the lag operation. We drop the first observation of `mu2_hat` and squared the results. Next, we remove the last observation of `mu2_hat_1` using the subtraction operator combined with a call to the `NROW` function on `return_mu`. Now, both contain $688$ observations and we can estimate a standard linear model.

```{r}
mu2_hat  <- return_mu[-1]^2
mu2_hat_1 <- return_mu[-NROW(return_mu)]^2
arch_model <- lm(mu2_hat ~ mu2_hat_1)
```

```{r, results='asis', echo=FALSE, warning=FALSE, message=FALSE, tidy=TRUE}
stargazer(type = "html",arch_model, single.row = TRUE, header = FALSE)
```


\newpage

## Chapter 13: Pooling Cross Sections across Time: Simple Panel Data Methods

###**`Example 13.7:` Effect of Drunk Driving Laws on Traffic Fatalities**

Wooldridge collected these data from two sources, the 1992 _Statistical Abstract of the United States_ (Tables 1009, 1012) and _A Digest of State Alcohol-Highway Safety Related Legislation_, 1985 and 1990, published by the U.S. National Highway Traffic Safety Administration. 
$$\widehat{\Delta{dthrte}} = \beta_0 + \Delta{open} + \Delta{admin}$$

```{r, message=FALSE, eval=FALSE}
data("traffic1")
?traffic1
```
```{r}
DD_model <- lm(cdthrte ~ copen + cadmn, data = traffic1)
```

```{r, results='asis', echo=FALSE, warning=FALSE, message=FALSE, tidy=TRUE}
stargazer(type = "html",DD_model,  single.row = TRUE, header = FALSE)
```

\newpage

## Chapter 14: Advanced Panel Data Methods

###**`Example 14.1:` Effect of Job Training on Firm Scrap Rates**

In this section, we will estimate a linear panel modeg using the `plm` function from the `plm: Linear Models for Panel Data` package. See the bibliography for more information.

```{r, tidy=TRUE, warning=FALSE, message=FALSE}
library(plm)
data("jtrain")
scrap_panel <- plm(lscrap ~ d88 + d89 + grant + grant_1, data = jtrain,
            index = c('fcode','year'), model = 'within', effect ='individual')
```

```{r, results='asis', echo=FALSE, warning=FALSE, message=FALSE, tidy=TRUE}
stargazer(type = "html",scrap_panel,  single.row = TRUE, header = FALSE)
```

\newpage

## Chapter 15: Instrumental Variables Estimation and Two Stage Least Squares

###**`Example 15.1:` Estimating the Return to Education for Married Women**

T.A. Mroz (1987), _The Sensitivity of an Empirical Model of Married Women's Hours of Work to Economic and Statistical Assumptions_, Econometrica 55, 765-799. Professor Ernst R. Berndt, of MIT, kindly provided the data, which he obtained from Professor Mroz.

$$log(wage) = \beta_0 + \beta_1educ + \mu$$

```{r, message=FALSE, eval=FALSE}
data("mroz")
?mroz
```
```{r}
wage_educ_model <- lm(lwage ~ educ, data = mroz)
```


$$\widehat{educ} = \beta_0 + \beta_1fatheduc$$

We run the typical linear model, but notice the use of the `subset` argument. `inlf` is a binary variable in which a value of 1 means they are "In the Labor Force". By sub-setting the `mroz` data.frame by observations in which `inlf==1`, only working women will be in the sample.

```{r}
fatheduc_model <- lm(educ ~ fatheduc, data = mroz, subset = (inlf==1))
```

In this section, we will perform an **Instrumental-Variable Regression**, using the  `ivreg` function in the `AER (Applied Econometrics with R)` package. See the bibliography for more information.

```{r, message=FALSE}
library("AER")
wage_educ_IV <- ivreg(lwage ~ educ | fatheduc, data = mroz)
```

```{r, results='asis', echo=FALSE, warning=FALSE, message=FALSE, tidy=TRUE}
stargazer(type = "html",wage_educ_model, fatheduc_model, wage_educ_IV, single.row = TRUE, header = FALSE)
```


\newpage

###**`Example 15.2:` Estimating the Return to Education for Men**

Data from M. Blackburn and D. Neumark (1992), _Unobserved Ability, Efficiency Wages, and Interindustry Wage Differentials_, Quarterly Journal of Economics 107, 1421-1436. Professor Neumark kindly provided the data, of which Wooldridge uses the data for 1980.

$$\widehat{educ} = \beta_0 + sibs$$

```{r, message=FALSE, eval=FALSE}
data("wage2")
?wage2
```{r}
educ_sibs_model <- lm(educ ~ sibs, data = wage2)
```


$$\widehat{log(wage)} = \beta_0 + educ$$

Again, estimate the model using the  `ivreg` function in the `AER (Applied Econometrics with R)` package.

```{r, message=FALSE}
library("AER")
educ_sibs_IV <- ivreg(lwage ~ educ | sibs, data = wage2)
```

```{r, results='asis', echo=FALSE, warning=FALSE, message=FALSE, tidy=TRUE}
stargazer(type = "html",educ_sibs_model, educ_sibs_IV, wage_educ_IV,  single.row = TRUE, header = FALSE)
```

\newpage

###**`Example 15.5:` Return to Education for Working Women**


$$\widehat{log(wage)} = \beta_0 + \beta_1educ + \beta_2exper + \beta_3exper^2$$

Use the  `ivreg` function in the `AER (Applied Econometrics with R)` package to estimate.

```{r, tidy=TRUE}
data("mroz")
wage_educ_exper_IV <- ivreg(lwage ~ educ + exper + expersq | exper + expersq + motheduc + fatheduc, data = mroz)
```

```{r, results='asis', echo=FALSE, warning=FALSE, message=FALSE, tidy=TRUE, echo=FALSE}
stargazer(type = "html",wage_educ_exper_IV,  single.row = TRUE, header = FALSE)
```

\newpage

## Chapter 16: Simultaneous Equations Models

###**`Example 16.4:` INFLATION AND OPENNESS**

Data from D. Romer (1993), _Openness and Inflation: Theory and Evidence_, Quarterly Journal of Economics 108, 869-903. The data are included in the article.

$$inf = \beta_{10} + \alpha_1open + \beta_{11}log(pcinc) + \mu_1$$
$$open = \beta_{20} + \alpha_2inf + \beta_{21}log(pcinc) + \beta_{22}log(land) + \mu_2$$

###**`Example 16.6:` INFLATION AND OPENNESS**

$$\widehat{open} = \beta_0 + \beta_{1}log(pcinc) + \beta_{2}log(land)$$


```{r, message=FALSE, eval=FALSE}
data("openness")
?openness
```{r}
open_model <-lm(open ~ lpcinc + lland, data = openness)
```

$$\widehat{inf} = \beta_0 + \beta_{1}open + \beta_{2}log(pcinc)$$

Use the  `ivreg` function in the `AER (Applied Econometrics with R)` package to estimate.

```{r}
library(AER)
inflation_IV <- ivreg(inf ~ open + lpcinc | lpcinc + lland, data = openness)
```

```{r, results='asis', echo=FALSE, warning=FALSE, message=FALSE, tidy=TRUE}
stargazer(type = "html",open_model, inflation_IV,  single.row = TRUE, header = FALSE)
```


\newpage


## Chapter 17: Limited Dependent Variable Models and Sample Selection Corrections

###**`Example 17.3:` POISSON REGRESSION FOR NUMBER OF ARRESTS**

```{r, tidy=TRUE, warning=FALSE}
data("crime1")
```

Sometimes, when estimating a model with many variables, defining a `model` object containing the formula makes for much cleaner code. 

```{r, tidy=TRUE, warning=FALSE}
formula <- (narr86 ~ pcnv + avgsen + tottime + ptime86 + qemp86 + inc86 + black + hispan + born60)
```

Then, pass the `formula` object into the `lm` function, and define the `data` argument as usual. 

```{r, tidy=TRUE, warning=FALSE}
econ_crime_model <- lm(formula, data = crime1)
```

To estimate the `poisson` regression, use the general linear model function `glm` and define the `family` argument as `poisson`.

```{r, tidy=TRUE, warning=FALSE}
econ_crim_poisson <- glm(formula, data = crime1, family = poisson)
```

Use the `stargazer` package to easily compare diagnostic tables of both models.

```{r, results='asis', echo=FALSE, warning=FALSE, message=FALSE, tidy=TRUE}
stargazer(type = "html",econ_crime_model, econ_crim_poisson,  single.row = TRUE, header = FALSE)
```


\newpage

## Chapter 18: Advanced Time Series Topics

###**`Example 18.8:` FORECASTING THE U.S. UNEMPLOYMENT RATE**

Data from _Economic Report of the President, 2004_, Tables B-42 and B-64.

```{r, message=FALSE, eval=FALSE}
data("phillips")
?phillips
```

$$\widehat{unemp_t} = \beta_0 + \beta_1unem_{t-1}$$

Estimate the linear model in the usual way and note the use of the `subset` argument to define data equal to and before the year 1996.

```{r}
phillips_train <- subset(phillips, year <= 1996)

unem_AR1 <- lm(unem ~ unem_1, data = phillips_train)
```

$$\widehat{unemp_t} = \beta_0 + \beta_1unem_{t-1} + \beta_2inf_{t-1}$$

```{r}
unem_inf_VAR1 <- lm(unem ~ unem_1 + inf_1, data = phillips_train)
```

```{r, results='asis', echo=FALSE, warning=FALSE, message=FALSE}
stargazer(type = "html",unem_AR1, unem_inf_VAR1,  single.row = TRUE, header = FALSE)
```

Now, use the `subset` argument to create our testing data set containing observation after 1996.
Next, pass the both the model object and the test set to the `predict` function for both models.
Finally, `cbind` or "column bind" both forecasts as well as the year and unemployment rate of the test set.

```{r, warning=FALSE, message=FALSE, echo=TRUE}
phillips_test <- subset(phillips, year >= 1997)

AR1_forecast <- predict.lm(unem_AR1, newdata = phillips_test)
VAR1_forecast <- predict.lm(unem_inf_VAR1, newdata = phillips_test)

kable(cbind(phillips_test[ ,c("year", "unem")], AR1_forecast, VAR1_forecast))
```



\newpage

# Appendix


###Using R for Introductory Econometrics

This is an excellent open source complimentary text to "Introductory Econometrics" by Jeffrey M. Wooldridge and should be your number one resource. This excerpt from the book's website:

>  This book introduces the popular, powerful and free programming language and software package R with a focus on the implementation of standard tools and methods used in econometrics. Unlike other books on similar topics, it does not attempt to provide a self-contained discussion of econometric models and methods. Instead, it builds on the excellent and popular textbook "Introductory Econometrics" by Jeffrey M. Wooldridge.

Hess, Florian. _Using R for Introductory Econometrics_. ISBN: 978-1-523-28513-6, CreateSpace Independent Publishing Platform, 2016, Dusseldorf, Germany. 

[url: http://www.urfie.net/](http://www.urfie.net/).


###Applied Econometrics with R

From the publisher's website:

>    This is the first book on applied econometrics using the R system for statistical computing and graphics. It presents hands-on examples for a wide range of econometric models, from classical linear regression models for cross-section, time series or panel data and the common non-linear models of microeconometrics such as logit, probit and tobit models, to recent semiparametric extensions. In addition, it provides a chapter on programming, including simulations, optimization, and an introduction to R tools enabling reproducible econometric research. An R package accompanying this book, AER, is available from the Comprehensive R Archive Network (CRAN) at https://CRAN.R-project.org/package=AER.

Kleiber, Christian  and Achim Zeileis. _Applied Econometrics with R_. ISBN 978-0-387-77316-2,
Springer-Verlag, 2008, New York. [https://www.springer.com/us/book/9780387773162](https://www.springer.com/us/book/9780387773162)

\newpage

## Bibliography

Yves Croissant, Giovanni Millo (2008). _Panel Data Econometrics in R: The plm Package_. Journal of Statistical Software 27(2). URL www.jstatsoft.org/v27/i02/.


Marek Hlavac (2015). _stargazer: Well-Formatted Regression and Summary Statistics Tables_. R package version 5.2. https://CRAN.R-project.org/package=stargazer


Christian Kleiber and Achim Zeileis (2008). _Applied Econometrics with R_. New York:
Springer-Verlag. ISBN 978-0-387-77316-2. URL https://CRAN.R-project.org/package=AER


Franz Mohr (2015). _prais: Prais-Winsten Estimation Procedure for AR(1) Serial Correlation_. R package version 0.1.1. https://CRAN.R-project.org/package=prais


R Core Team (2018). _R: A language and environment for statistical computing_. 
R Foundation for Statistical Computing, Vienna, Austria. URL https://www.R-project.org/.


Hadley Wickham and Winston Chang (2016). _devtools: Tools to Make Developing R Packages Easier_. R package version 1.12.0. https://CRAN.R-project.org/package=devtools


Hadley Wickham. _testthat: Get Started with Testing_. R package version 1.0.2. https://CRAN.R-project.org/package=testthat


Jeffrey M. Wooldridge (2016). _Introductory Econometrics: A Modern Approach_. 
Mason, Ohio :South-Western Cengage Learning.


Yihui Xie (2018). _knitr: A General-Purpose Package for Dynamic Report Generation in R_. R package version 1.16. https://CRAN.R-project.org/package=knitr

 







